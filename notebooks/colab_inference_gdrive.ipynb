{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NCF Movie Recommender - Model Inference on Colab\n",
    "\n",
    "This notebook demonstrates how to use trained NCF/NeuMF+ models for movie recommendations.\n",
    "\n",
    "**Features:**\n",
    "- Load trained models from Google Drive\n",
    "- Generate top-K movie recommendations for users\n",
    "- Predict scores for user-item pairs\n",
    "- Handle cold-start scenarios (new movies)\n",
    "- Display movie titles and genres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Setup - Mount Google Drive\n\nThis notebook expects your trained models and data to be in Google Drive.\n\n**Required structure in Google Drive:**\n```\nMyDrive/\n‚îî‚îÄ‚îÄ NCF-Movie-Recommender/\n    ‚îú‚îÄ‚îÄ data/                    # Processed data files\n    ‚îÇ   ‚îú‚îÄ‚îÄ mappings.pkl         # User/item mappings\n    ‚îÇ   ‚îú‚îÄ‚îÄ item_synopsis_embeddings.npy\n    ‚îÇ   ‚îî‚îÄ‚îÄ ...\n    ‚îú‚îÄ‚îÄ datasets/                # Raw datasets\n    ‚îÇ   ‚îî‚îÄ‚îÄ movies_metadata.csv  # Movie titles and info\n    ‚îî‚îÄ‚îÄ experiments/\n        ‚îî‚îÄ‚îÄ trained_models/      # Trained model checkpoints\n            ‚îú‚îÄ‚îÄ NeuMFPlus_genre_synopsis_best.pt\n            ‚îî‚îÄ‚îÄ ...\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "print(\"‚úÖ Google Drive mounted successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configure Paths\n",
    "\n",
    "Update these paths to match your Google Drive structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# @title Configure paths\nimport os\n\n# @markdown **Base path for NCF-Movie-Recommender project:**\nGDRIVE_BASE = \"/content/drive/MyDrive/NCF-Movie-Recommender\"  # @param {type:\"string\"}\n\n# Paths relative to your Google Drive base\nDATA_DIR = os.path.join(GDRIVE_BASE, \"data\")\nDATASETS_DIR = os.path.join(GDRIVE_BASE, \"datasets\")\nMODELS_DIR = os.path.join(GDRIVE_BASE, \"experiments\", \"trained_models\")\n\nprint(f\"üìÅ Base directory: {GDRIVE_BASE}\")\nprint(f\"üìÅ Data directory: {DATA_DIR}\")\nprint(f\"üìÅ Datasets directory: {DATASETS_DIR}\")\nprint(f\"üìÅ Models directory: {MODELS_DIR}\")\n\n# Verify directories exist\nif os.path.exists(DATA_DIR):\n    data_files = os.listdir(DATA_DIR)\n    print(f\"\\n‚úÖ Data directory found! Files: {len(data_files)}\")\nelse:\n    print(f\"\\n‚ùå Data directory not found: {DATA_DIR}\")\n\nif os.path.exists(DATASETS_DIR):\n    datasets_files = os.listdir(DATASETS_DIR)\n    print(f\"‚úÖ Datasets directory found! Files: {len(datasets_files)}\")\nelse:\n    print(f\"‚ùå Datasets directory not found: {DATASETS_DIR}\")\n\nif os.path.exists(MODELS_DIR):\n    model_files = [f for f in os.listdir(MODELS_DIR) if f.endswith('.pt')]\n    print(f\"‚úÖ Models directory found! Checkpoints: {len(model_files)}\")\n    if model_files:\n        print(\"\\nAvailable models:\")\n        for f in sorted(model_files):\n            print(f\"  ‚Ä¢ {f}\")\nelse:\n    print(f\"\\n‚ùå Models directory not found: {MODELS_DIR}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Install Dependencies\n",
    "\n",
    "Install required Python packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Install dependencies\n",
    "!pip install -q torch numpy pandas sentence-transformers tqdm\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "print(\"‚úÖ Dependencies installed!\")\n",
    "print(f\"   PyTorch: {torch.__version__}\")\n",
    "print(f\"   CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Set device\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"   Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Model Architecture\n",
    "\n",
    "This section defines the NeuMF+ model architecture to match your trained checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Define NeuMF+ Model\n",
    "import torch.nn as nn\n",
    "\n",
    "class ContentEncoder(nn.Module):\n",
    "    \"\"\"Encode content features (genre + synopsis) into embeddings.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_genres: int, genre_embed_dim: int = 64,\n",
    "                 synopsis_embed_dim: int = 384, content_embed_dim: int = 256,\n",
    "                 dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.genre_encoder = nn.Sequential(\n",
    "            nn.Linear(num_genres, genre_embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        \n",
    "        self.synopsis_projection = nn.Sequential(\n",
    "            nn.Linear(synopsis_embed_dim, synopsis_embed_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        \n",
    "        combined_dim = genre_embed_dim + synopsis_embed_dim // 2\n",
    "        self.content_encoder = nn.Sequential(\n",
    "            nn.Linear(combined_dim, content_embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "    \n",
    "    def forward(self, genre_features, synopsis_embeddings):\n",
    "        genre_embed = self.genre_encoder(genre_features)\n",
    "        synopsis_embed = self.synopsis_projection(synopsis_embeddings)\n",
    "        combined = torch.cat([genre_embed, synopsis_embed], dim=-1)\n",
    "        return self.content_encoder(combined)\n",
    "\n",
    "\n",
    "class GatedFusion(nn.Module):\n",
    "    \"\"\"Gated fusion for CF and content embeddings.\"\"\"\n",
    "    \n",
    "    def __init__(self, cf_dim: int, content_dim: int, hidden_dim: int = 64, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.gate_network = nn.Sequential(\n",
    "            nn.Linear(cf_dim + content_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, cf_embed, content_embed):\n",
    "        combined = torch.cat([cf_embed, content_embed], dim=-1)\n",
    "        gate = self.gate_network(combined)\n",
    "        \n",
    "        if cf_embed.shape[-1] != content_embed.shape[-1]:\n",
    "            if cf_embed.shape[-1] > content_embed.shape[-1]:\n",
    "                target_dim = cf_embed.shape[-1]\n",
    "                if not hasattr(self, '_content_proj'):\n",
    "                    self._content_proj = nn.Linear(content_embed.shape[-1], target_dim).to(cf_embed.device)\n",
    "                content_embed = self._content_proj(content_embed)\n",
    "            else:\n",
    "                target_dim = content_embed.shape[-1]\n",
    "                if not hasattr(self, '_cf_proj'):\n",
    "                    self._cf_proj = nn.Linear(cf_embed.shape[-1], target_dim).to(cf_embed.device)\n",
    "                cf_embed = self._cf_proj(cf_embed)\n",
    "        \n",
    "        fused = gate * cf_embed + (1 - gate) * content_embed\n",
    "        return fused, gate\n",
    "\n",
    "\n",
    "class NeuMFPlus(nn.Module):\n",
    "    \"\"\"Neural Matrix Factorization with content features.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_users: int, num_items: int,\n",
    "                 embed_dim: int = 32, hidden_dims: List[int] = [128, 64],\n",
    "                 num_genres: int = 19, use_genre: bool = True,\n",
    "                 use_synopsis: bool = True, use_gated_fusion: bool = True,\n",
    "                 synopsis_embed_dim: int = 384, content_embed_dim: int = 256,\n",
    "                 dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_genres = num_genres\n",
    "        self.use_genre = use_genre\n",
    "        self.use_synopsis = use_synopsis\n",
    "        self.use_gated_fusion = use_gated_fusion\n",
    "        self.synopsis_embed_dim = synopsis_embed_dim\n",
    "        self.content_embed_dim = content_embed_dim\n",
    "        \n",
    "        # GMF part\n",
    "        self.user_gmf_embed = nn.Embedding(num_users, embed_dim)\n",
    "        self.item_gmf_embed = nn.Embedding(num_items, embed_dim)\n",
    "        \n",
    "        # MLP part\n",
    "        self.user_mlp_embed = nn.Embedding(num_users, embed_dim)\n",
    "        self.item_mlp_embed = nn.Embedding(num_items, embed_dim)\n",
    "        \n",
    "        mlp_input_dim = embed_dim * 2\n",
    "        mlp_layers = []\n",
    "        for dim in hidden_dims:\n",
    "            mlp_layers.extend([\n",
    "                nn.Linear(mlp_input_dim, dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "            ])\n",
    "            mlp_input_dim = dim\n",
    "        self.mlp = nn.Sequential(*mlp_layers)\n",
    "        \n",
    "        # Content encoder\n",
    "        self.use_content = use_genre or use_synopsis\n",
    "        if self.use_content:\n",
    "            self.content_encoder = ContentEncoder(\n",
    "                num_genres=num_genres,\n",
    "                genre_embed_dim=64,\n",
    "                synopsis_embed_dim=synopsis_embed_dim,\n",
    "                content_embed_dim=content_embed_dim,\n",
    "                dropout=dropout,\n",
    "            )\n",
    "        \n",
    "        # Fusion layer\n",
    "        fusion_input_dim = embed_dim + hidden_dims[-1]\n",
    "        if self.use_content:\n",
    "            if use_gated_fusion:\n",
    "                self.gated_fusion = GatedFusion(\n",
    "                    cf_dim=embed_dim + hidden_dims[-1],\n",
    "                    content_dim=content_embed_dim,\n",
    "                    dropout=dropout,\n",
    "                )\n",
    "                fusion_input_dim = embed_dim + hidden_dims[-1]\n",
    "            else:\n",
    "                fusion_input_dim += content_embed_dim\n",
    "        \n",
    "        self.output_layer = nn.Linear(fusion_input_dim, 1)\n",
    "        \n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, (nn.Embedding, nn.Linear)):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "    \n",
    "    def forward(self, user_ids, item_ids, genre_features=None, synopsis_embeddings=None):\n",
    "        # GMF embeddings\n",
    "        user_gmf = self.user_gmf_embed(user_ids)\n",
    "        item_gmf = self.item_gmf_embed(item_ids)\n",
    "        gmf_output = user_gmf * item_gmf\n",
    "        \n",
    "        # MLP embeddings\n",
    "        user_mlp = self.user_mlp_embed(user_ids)\n",
    "        item_mlp = self.item_mlp_embed(item_ids)\n",
    "        mlp_input = torch.cat([user_mlp, item_mlp], dim=-1)\n",
    "        mlp_output = self.mlp(mlp_input)\n",
    "        \n",
    "        # Combine GMF + MLP\n",
    "        cf_output = torch.cat([gmf_output, mlp_output], dim=-1)\n",
    "        \n",
    "        # Add content features if available\n",
    "        if self.use_content:\n",
    "            content_features = []\n",
    "            if self.use_genre and genre_features is not None:\n",
    "                content_features.append(genre_features)\n",
    "            if self.use_synopsis and synopsis_embeddings is not None:\n",
    "                content_features.append(synopsis_embeddings)\n",
    "            \n",
    "            # Use zeros if features not provided\n",
    "            batch_size = user_ids.shape[0]\n",
    "            if not content_features:\n",
    "                if self.use_genre:\n",
    "                    content_features.append(torch.zeros(batch_size, self.num_genres).to(user_ids.device))\n",
    "                if self.use_synopsis:\n",
    "                    content_features.append(torch.zeros(batch_size, self.synopsis_embed_dim).to(user_ids.device))\n",
    "            \n",
    "            # For genre-only or synopsis-only, create dummy for the other\n",
    "            if len(content_features) == 1:\n",
    "                if self.use_genre:\n",
    "                    content_features.insert(0, torch.zeros(batch_size, self.synopsis_embed_dim).to(user_ids.device))\n",
    "                else:\n",
    "                    content_features.append(torch.zeros(batch_size, self.num_genres).to(user_ids.device))\n",
    "            \n",
    "            content_input = torch.cat(content_features, dim=-1)\n",
    "            content_embed = self.content_encoder(\n",
    "                content_input[:, :self.num_genres],\n",
    "                content_input[:, self.num_genres:]\n",
    "            )\n",
    "            \n",
    "            if self.use_gated_fusion:\n",
    "                cf_output, gate = self.gated_fusion(cf_output, content_embed)\n",
    "            else:\n",
    "                cf_output = torch.cat([cf_output, content_embed], dim=-1)\n",
    "        \n",
    "        output = self.output_layer(cf_output)\n",
    "        return output\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, checkpoint_path: str, device: str = 'cpu'):\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "        config = checkpoint['model_config']\n",
    "        \n",
    "        model = cls(\n",
    "            num_users=config['num_users'],\n",
    "            num_items=config['num_items'],\n",
    "            embed_dim=config.get('embed_dim', 32),\n",
    "            hidden_dims=config.get('hidden_dims', [128, 64]),\n",
    "            num_genres=config.get('num_genres', 19),\n",
    "            use_genre=config.get('use_genre', True),\n",
    "            use_synopsis=config.get('use_synopsis', True),\n",
    "            use_gated_fusion=config.get('use_gated_fusion', True),\n",
    "            synopsis_embed_dim=config.get('synopsis_embed_dim', 384),\n",
    "            content_embed_dim=config.get('content_embed_dim', 256),\n",
    "            dropout=config.get('dropout', 0.1),\n",
    "        )\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.to(device)\n",
    "        return model, checkpoint\n",
    "\n",
    "print(\"‚úÖ Model architecture defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Data and Mappings\n",
    "\n",
    "Load the processed data files including mappings, genre features, and movie metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# @title Load mappings and features\n\n# Load mappings\nmappings_path = os.path.join(DATA_DIR, \"mappings.pkl\")\nwith open(mappings_path, 'rb') as f:\n    mappings = pickle.load(f)\n\nNUM_USERS = mappings['num_users']\nNUM_ITEMS = mappings['num_items']\nNUM_GENRES = mappings['num_genres']\nGENRE_NAMES = mappings.get('genre_names', [])\n\nprint(f\"‚úÖ Mappings loaded!\")\nprint(f\"   Users: {NUM_USERS:,}\")\nprint(f\"   Items: {NUM_ITEMS:,}\")\nprint(f\"   Genres: {NUM_GENRES}\")\nif GENRE_NAMES:\n    print(f\"   Genre names: {GENRE_NAMES}\")\n\n# Load genre features (if available)\ngenre_path = os.path.join(DATA_DIR, \"item_genre_features.npy\")\nif os.path.exists(genre_path):\n    GENRE_FEATURES = np.load(genre_path)\n    print(f\"\\n‚úÖ Genre features loaded: {GENRE_FEATURES.shape}\")\nelse:\n    GENRE_FEATURES = None\n    print(f\"\\n‚ö†Ô∏è  Genre features not found: {genre_path}\")\n\n# Load synopsis embeddings\n# Note: file uses plural \"embeddings\"\nsynopsis_path = os.path.join(DATA_DIR, \"item_synopsis_embeddings.npy\")\nif os.path.exists(synopsis_path):\n    SYNOPSIS_EMBEDDINGS = np.load(synopsis_path)\n    print(f\"‚úÖ Synopsis embeddings loaded: {SYNOPSIS_EMBEDDINGS.shape}\")\nelse:\n    SYNOPSIS_EMBEDDINGS = None\n    print(f\"‚ö†Ô∏è  Synopsis embeddings not found: {synopsis_path}\")\n\n# Load movie metadata for display (from datasets directory)\nmetadata_path = os.path.join(DATASETS_DIR, \"movies_metadata.csv\")\nif os.path.exists(metadata_path):\n    movies_df = pd.read_csv(metadata_path)\n    # Filter for valid IDs\n    movies_df['id'] = pd.to_numeric(movies_df['id'], errors='coerce')\n    movies_df = movies_df[movies_df['id'].notna()]\n    movies_df['id'] = movies_df['id'].astype(int)\n    movies_df = movies_df.set_index('id')\n    print(f\"\\n‚úÖ Movie metadata loaded: {len(movies_df):,} movies\")\nelse:\n    movies_df = None\n    print(f\"\\n‚ö†Ô∏è  Movie metadata not found: {metadata_path}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Load Trained Model\n\nSelect and load one of your trained models.\n\n**Available Models:**\n| Model | Description | Features |\n|-------|-------------|----------|\n| `NeuMF_best.pt` | Baseline | Collaborative Filtering only |\n| `NeuMFPlus_genre_best.pt` | Genre-enhanced | CF + Genre features |\n| `NeuMFPlus_genre_synopsis_bestt.pt` | Full model | CF + Genre + Synopsis |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# @title Load trained model\n# @markdown Select the model checkpoint to load:\n\nimport ipywidgets as widgets\nfrom IPython.display import display, HTML\n\n# Get available models\navailable_models = [f for f in os.listdir(MODELS_DIR) if f.endswith('.pt')]\n\n# Model descriptions\nMODEL_INFO = {\n    'NeuMF_best.pt': {\n        'name': 'NeuMF (Baseline)',\n        'description': 'Collaborative Filtering only - no content features',\n        'features': 'User-Item interactions only'\n    },\n    'NeuMFPlus_genre_best.pt': {\n        'name': 'NeuMF+ (Genre)',\n        'description': 'CF + Genre features',\n        'features': 'User-Item + Movie genres'\n    },\n    'NeuMFPlus_genre_synopsis_bestt.pt': {\n        'name': 'NeuMF+ (Genre + Synopsis)',\n        'description': 'CF + Genre + Synopsis features (Full Model)',\n        'features': 'User-Item + Genres + Movie synopsis'\n    }\n}\n\nif not available_models:\n    print(f\"‚ùå No models found in {MODELS_DIR}\")\nelse:\n    # Create dropdown with model descriptions\n    model_options = [(f\"{m}  ({MODEL_INFO.get(m, {}).get('name', m)})\", m) for m in sorted(available_models)]\n    \n    model_dropdown = widgets.Dropdown(\n        options=model_options,\n        description='Select model:',\n        style={'description_width': 'initial'},\n    )\n    display(model_dropdown)\n    \n    # Model info display\n    info_out = widgets.Output()\n    display(info_out)\n    \n    def show_model_info(change):\n        with info_out:\n            info_out.clear_output()\n            model_name = change['new']\n            info = MODEL_INFO.get(model_name, {})\n            if info:\n                print(f\"üìã {info.get('name', model_name)}\")\n                print(f\"   {info.get('description', '')}\")\n                print(f\"   Features: {info.get('features', 'N/A')}\")\n    \n    model_dropdown.observe(show_model_info, names='value')\n    # Show initial info\n    show_model_info({'new': model_dropdown.value})\n    \n    # Load button\n    load_btn = widgets.Button(description='Load Model', button_style='primary')\n    display(load_btn)\n    \n    # Output area\n    out = widgets.Output()\n    display(out)\n    \n    def load_model(b):\n        with out:\n            out.clear_output()\n            model_name = model_dropdown.value\n            checkpoint_path = os.path.join(MODELS_DIR, model_name)\n            \n            print(f\"Loading model from: {model_name}\")\n            print(f\"Path: {checkpoint_path}\")\n            \n            global model, checkpoint, model_config\n            model, checkpoint = NeuMFPlus.load(checkpoint_path, device=DEVICE)\n            model.eval()\n            model_config = checkpoint['model_config']\n            \n            print(\"\\n\" + \"=\"*70)\n            print(\"MODEL CONFIGURATION\")\n            print(\"=\"*70)\n            print(f\"use_genre: {model_config.get('use_genre')}\")\n            print(f\"use_synopsis: {model_config.get('use_synopsis')}\")\n            print(f\"use_gated_fusion: {model_config.get('use_gated_fusion')}\")\n            print(f\"\\nParameters: {sum(p.numel() for p in model.parameters()):,}\")\n            \n            if 'metrics' in checkpoint:\n                print(\"\\nValidation Metrics:\")\n                for k, v in checkpoint['metrics'].items():\n                    if isinstance(v, (int, float)):\n                        print(f\"  {k}: {v:.4f}\")\n            \n            # Show what features are needed\n            print(\"\\n\" + \"-\"*70)\n            print(\"REQUIRED FEATURES FOR INFERENCE:\")\n            if model_config.get('use_genre'):\n                print(\"  ‚úÖ Genre features (item_genre_features.npy)\")\n            else:\n                print(\"  ‚ùå Genre features NOT needed\")\n            if model_config.get('use_synopsis'):\n                print(\"  ‚úÖ Synopsis embeddings (item_synopsis_embeddings.npy)\")\n            else:\n                print(\"  ‚ùå Synopsis embeddings NOT needed\")\n            print(\"-\"*70)\n            \n            print(\"\\n‚úÖ Model loaded successfully!\")\n    \n    load_btn.on_click(load_model)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Helper Functions\n",
    "\n",
    "Define helper functions for prediction and recommendation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# @title Define helper functions\n\ndef get_movie_title(item_id: int, movies_df: Optional[pd.DataFrame] = None) -> str:\n    \"\"\"Get movie title for item ID.\"\"\"\n    if movies_df is None:\n        return f\"Movie {item_id}\"\n    \n    # Try to get title from movies_df\n    if item_id in movies_df.index:\n        title = movies_df.loc[item_id, 'title']\n        return title if pd.notna(title) else f\"Movie {item_id}\"\n    \n    return f\"Movie {item_id}\"\n\n\ndef parse_genres(genres_str: str) -> list:\n    \"\"\"Parse genres from JSON string.\"\"\"\n    import json\n    import ast\n    \n    if pd.isna(genres_str) or genres_str == \"\":\n        return []\n    \n    try:\n        genres = json.loads(genres_str)\n        return [g['name'] for g in genres]\n    except:\n        try:\n            genres = ast.literal_eval(genres_str)\n            return [g['name'] for g in genres]\n        except:\n            return []\n\n\ndef get_movie_genres(item_id: int, movies_df: Optional[pd.DataFrame] = None) -> str:\n    \"\"\"Get genres for item ID.\"\"\"\n    if movies_df is None or GENRE_FEATURES is None:\n        return \"Unknown\"\n    \n    if item_id in movies_df.index:\n        genres_str = movies_df.loc[item_id, 'genres']\n        genres = parse_genres(genres_str)\n        return ', '.join(genres) if genres else \"Unknown\"\n    \n    # Use genre features if available\n    if item_id < len(GENRE_FEATURES):\n        genre_indices = [i for i, g in enumerate(GENRE_FEATURES[item_id]) if g == 1]\n        if genre_indices and GENRE_NAMES:\n            return ', '.join([GENRE_NAMES[i] for i in genre_indices if i < len(GENRE_NAMES)])\n    \n    return \"Unknown\"\n\n\ndef predict_score(model, user_id: int, item_id: int, \n                genre_vector: Optional[np.ndarray] = None,\n                synopsis_embedding: Optional[np.ndarray] = None,\n                device: str = DEVICE) -> float:\n    \"\"\"Predict score for a user-item pair.\"\"\"\n    model.eval()\n    \n    user_tensor = torch.LongTensor([user_id]).to(device)\n    item_tensor = torch.LongTensor([item_id]).to(device)\n    \n    kwargs = {}\n    if genre_vector is not None:\n        kwargs['genre_features'] = torch.FloatTensor([genre_vector]).to(device)\n    if synopsis_embedding is not None:\n        kwargs['synopsis_embeddings'] = torch.FloatTensor([synopsis_embedding]).to(device)\n    \n    with torch.no_grad():\n        logits = model(user_tensor, item_tensor, **kwargs)\n        score = torch.sigmoid(logits).squeeze(-1).item()\n    \n    return score\n\n\ndef recommend(model, user_id: int, k: int = 10,\n             item_genre_features: Optional[np.ndarray] = None,\n             item_synopsis_embeddings: Optional[np.ndarray] = None,\n             seen_items: Optional[List[int]] = None,\n             device: str = DEVICE) -> List[Dict]:\n    \"\"\"Recommend top-K items for a user.\"\"\"\n    model.eval()\n    num_items = model.num_items\n    \n    candidate_items = list(range(num_items))\n    if seen_items is not None:\n        candidate_items = [item for item in candidate_items if item not in seen_items]\n    \n    user_tensor = torch.LongTensor([user_id] * len(candidate_items)).to(device)\n    item_tensor = torch.LongTensor(candidate_items).to(device)\n    \n    kwargs = {}\n    if item_genre_features is not None:\n        kwargs['genre_features'] = torch.FloatTensor(item_genre_features[candidate_items]).to(device)\n    if item_synopsis_embeddings is not None:\n        kwargs['synopsis_embeddings'] = torch.FloatTensor(item_synopsis_embeddings[candidate_items]).to(device)\n    \n    with torch.no_grad():\n        logits = model(user_tensor, item_tensor, **kwargs)\n        scores = torch.sigmoid(logits).squeeze(-1).cpu().numpy()\n    \n    top_indices = np.argsort(scores)[::-1][:k]\n    \n    recommendations = []\n    for idx in top_indices:\n        item_id = int(candidate_items[idx])\n        recommendations.append({\n            'item_id': item_id,\n            'score': float(scores[idx]),\n            'rank': len(recommendations) + 1,\n            'title': get_movie_title(item_id, movies_df),\n            'genres': get_movie_genres(item_id, movies_df),\n        })\n    \n    return recommendations\n\n\ndef load_multiple_models(model_paths: Dict[str, str], device: str = DEVICE) -> Dict[str, tuple]:\n    \"\"\"Load multiple models for comparison.\"\"\"\n    loaded = {}\n    for name, path in model_paths.items():\n        try:\n            model_obj, checkpoint = NeuMFPlus.load(path, device=device)\n            model_obj.eval()\n            loaded[name] = (model_obj, checkpoint)\n            print(f\"‚úÖ Loaded: {name}\")\n        except Exception as e:\n            print(f\"‚ùå Failed to load {name}: {e}\")\n    return loaded\n\nprint(\"‚úÖ Helper functions defined!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7.1 Model Comparison\n\nCompare predictions from different models to see how content features affect recommendations.\n\n**Models available:**\n- **NeuMF (Baseline)**: Uses only user-item interaction patterns\n- **NeuMF+ (Genre)**: Adds genre information for better content-aware recommendations  \n- **NeuMF+ (Genre + Synopsis)**: Full model with both genre and synopsis features"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# @title Compare predictions from different models\n\n# Define model paths for comparison\nMODEL_PATHS = {\n    'Baseline (NeuMF)': os.path.join(MODELS_DIR, 'NeuMF_best.pt'),\n    'Genre Only': os.path.join(MODELS_DIR, 'NeuMFPlus_genre_best.pt'),\n    'Genre + Synopsis': os.path.join(MODELS_DIR, 'NeuMFPlus_genre_synopsis_bestt.pt'),\n}\n\n# @markdown Enter user ID and item ID to compare predictions:\ncompare_user_id = 100  # @param {type:\"integer\"}\ncompare_item_id = 500  # @param {type:\"integer\"}\n\nif compare_user_id >= NUM_USERS:\n    print(f\"‚ùå Invalid user ID. Must be less than {NUM_USERS}.\")\nelif compare_item_id >= NUM_ITEMS:\n    print(f\"‚ùå Invalid item ID. Must be less than {NUM_ITEMS}.\")\nelse:\n    print(\"Loading models for comparison...\")\n    print(\"-\"*70)\n    \n    loaded_models = load_multiple_models(MODEL_PATHS, device=DEVICE)\n    \n    if loaded_models:\n        print(\"\\n\" + \"=\"*70)\n        print(f\"MODEL COMPARISON: User {compare_user_id} ‚Üí Item {compare_item_id}\")\n        print(\"=\"*70)\n        print(f\"\\nMovie: {get_movie_title(compare_item_id, movies_df)}\")\n        print(f\"Genres: {get_movie_genres(compare_item_id, movies_df)}\")\n        \n        print(f\"\\n{'Model':<25} {'Score':<10} {'Recommendation'}\")\n        print(\"-\"*70)\n        \n        # Prepare features\n        genre_vec = GENRE_FEATURES[compare_item_id] if GENRE_FEATURES is not None else None\n        synopsis_emb = SYNOPSIS_EMBEDDINGS[compare_item_id] if SYNOPSIS_EMBEDDINGS is not None else None\n        \n        for model_name, (model_obj, _) in loaded_models.items():\n            # Determine what features to pass based on model config\n            config = model_obj.use_genre, model_obj.use_synopsis\n            \n            model_genre = genre_vec if model_obj.use_genre else None\n            model_synopsis = synopsis_emb if model_obj.use_synopsis else None\n            \n            score = predict_score(model_obj, compare_user_id, compare_item_id, \n                                model_genre, model_synopsis)\n            \n            if score > 0.8:\n                recommendation = \"Strongly recommend\"\n            elif score > 0.6:\n                recommendation = \"Recommend\"\n            elif score > 0.4:\n                recommendation = \"Maybe\"\n            else:\n                recommendation = \"Not recommended\"\n            \n            print(f\"{model_name:<25} {score:.4f}     {recommendation}\")\n        \n        # Calculate differences\n        baseline_name = 'Baseline (NeuMF)'\n        full_name = 'Genre + Synopsis'\n        if baseline_name in loaded_models and full_name in loaded_models:\n            baseline_score = predict_score(loaded_models[baseline_name][0], compare_user_id, compare_item_id)\n            full_score = predict_score(loaded_models[full_name][0], compare_user_id, compare_item_id, \n                                      genre_vec, synopsis_emb)\n            diff = full_score - baseline_score\n            \n            print(\"\\n\" + \"-\"*70)\n            print(f\"IMPROVEMENT: {diff:+.4f} ({(diff/baseline_score)*100:+.1f}%)\")\n            if diff > 0:\n                print(\"‚ú® Content features improved the prediction!\")\n            elif diff < 0:\n                print(\"üìâ Baseline performed better for this item\")\n            else:\n                print(\"‚ûñ No significant difference\")\n    else:\n        print(\"\\n‚ö†Ô∏è  No models could be loaded. Please check the paths.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Example: Top-K Recommendations\n",
    "\n",
    "Get personalized movie recommendations for a user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Get top-K recommendations\n",
    "# @markdown Enter user ID and number of recommendations:\n",
    "\n",
    "user_id_rec = 100  # @param {type:\"integer\"}\n",
    "k_recommendations = 10  # @param {type:\"integer\", min:1, max:50}\n",
    "\n",
    "if user_id_rec >= NUM_USERS:\n",
    "    print(f\"‚ùå Invalid user ID. Must be less than {NUM_USERS}.\")\n",
    "else:\n",
    "    recommendations = recommend(\n",
    "        model, user_id_rec, k=k_recommendations,\n",
    "        item_genre_features=GENRE_FEATURES,\n",
    "        item_synopsis_embeddings=SYNOPSIS_EMBEDDINGS,\n",
    "    )\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(f\"TOP-{k_recommendations} RECOMMENDATIONS FOR USER {user_id_rec}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(f\"\\n{'Rank':<6} {'Score':<10} {'Title':<50} {'Genres'}\")\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    for rec in recommendations:\n",
    "        title = rec['title'][:47] + '...' if len(rec['title']) > 47 else rec['title']\n",
    "        print(f\"{rec['rank']:<6} {rec['score']:.4f}     {title:<50} {rec['genres']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Example: Compare Multiple Users\n",
    "\n",
    "See how different users would rate the same movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Compare predictions for multiple users\n",
    "# @markdown Enter item ID and list of user IDs to compare:\n",
    "\n",
    "item_id_compare = 500  # @param {type:\"integer\"}\n",
    "user_ids_compare = \"0, 50, 100, 500, 1000\"  # @param {type:\"string\"}\n",
    "\n",
    "try:\n",
    "    user_list = [int(u.strip()) for u in user_ids_compare.split(',')]\n",
    "except:\n",
    "    user_list = [0, 50, 100, 500, 1000]\n",
    "\n",
    "if item_id_compare >= NUM_ITEMS:\n",
    "    print(f\"‚ùå Invalid item ID. Must be less than {NUM_ITEMS}.\")\n",
    "else:\n",
    "    print(\"=\"*70)\n",
    "    print(f\"USER COMPARISON FOR ITEM: {get_movie_title(item_id_compare, movies_df)}\")\n",
    "    print(f\"Genres: {get_movie_genres(item_id_compare, movies_df)}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(f\"\\n{'User ID':<12} {'Score':<10} {'Prediction'}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    genre_vec = GENRE_FEATURES[item_id_compare] if GENRE_FEATURES is not None else None\n",
    "    synopsis_emb = SYNOPSIS_EMBEDDINGS[item_id_compare] if SYNOPSIS_EMBEDDINGS is not None else None\n",
    "    \n",
    "    for user_id in user_list:\n",
    "        if user_id >= NUM_USERS:\n",
    "            print(f\"{user_id:<12} (invalid user)\")\n",
    "            continue\n",
    "        \n",
    "        score = predict_score(model, user_id, item_id_compare, genre_vec, synopsis_emb)\n",
    "        \n",
    "        if score > 0.8:\n",
    "            prediction = \"Will love it!\"\n",
    "        elif score > 0.6:\n",
    "            prediction = \"Will probably like it\"\n",
    "        elif score > 0.4:\n",
    "            prediction = \"Maybe\"\n",
    "        else:\n",
    "            prediction = \"Probably not interested\"\n",
    "        \n",
    "        print(f\"{user_id:<12} {score:.4f}     {prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Advanced: Cold-Start Prediction for New Movies\n",
    "\n",
    "Predict how users would rate a completely new movie using only its content features (genres and synopsis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Cold-start prediction for a new movie\n",
    "# @markdown Enter movie details for prediction:\n",
    "\n",
    "new_user_id = 100  # @param {type:\"integer\"}\n",
    "new_movie_genres = \"Action,Sci-Fi\"  # @param {type:\"string\"}\n",
    "new_movie_synopsis = \"A group of astronauts discover a mysterious artifact on Mars that changes their understanding of humanity's place in the universe.\"  # @param {type:\"string\"}\n",
    "\n",
    "if new_user_id >= NUM_USERS:\n",
    "    print(f\"‚ùå Invalid user ID. Must be less than {NUM_USERS}.\")\n",
    "else:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    \n",
    "    # Load SBERT model for synopsis encoding\n",
    "    print(\"Loading Sentence-BERT model...\")\n",
    "    sbert = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    \n",
    "    # Encode genres\n",
    "    genre_list = [g.strip() for g in new_movie_genres.split(',')]\n",
    "    genre_vector = np.zeros(NUM_GENRES, dtype=np.float32)\n",
    "    \n",
    "    if GENRE_NAMES:\n",
    "        for genre in genre_list:\n",
    "            if genre in GENRE_NAMES:\n",
    "                idx = GENRE_NAMES.index(genre)\n",
    "                genre_vector[idx] = 1.0\n",
    "    \n",
    "    # Encode synopsis\n",
    "    synopsis_embedding = sbert.encode(new_movie_synopsis, show_progress_bar=False)\n",
    "    synopsis_embedding = np.array(synopsis_embedding, dtype=np.float32)\n",
    "    \n",
    "    # Use a placeholder item ID (last item as reference)\n",
    "    placeholder_item_id = NUM_ITEMS - 1\n",
    "    \n",
    "    # Predict\n",
    "    score = predict_score(\n",
    "        model, new_user_id, placeholder_item_id,\n",
    "        genre_vector=genre_vector,\n",
    "        synopsis_embedding=synopsis_embedding\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"COLD-START PREDICTION\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nUser ID: {new_user_id}\")\n",
    "    print(f\"\\nNew Movie:\")\n",
    "    print(f\"  Genres: {new_movie_genres}\")\n",
    "    print(f\"  Synopsis: {new_movie_synopsis[:100]}...\")\n",
    "    print(f\"\\n‚úÖ Predicted score: {score:.4f}\")\n",
    "    \n",
    "    if score > 0.7:\n",
    "        print(\"\\nüé¨ This user would likely enjoy this movie!\")\n",
    "    elif score > 0.5:\n",
    "        print(\"\\nüé¨ This user might be interested in this movie.\")\n",
    "    else:\n",
    "        print(\"\\nüé¨ This movie may not be a good fit for this user.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Interactive Recommendation Widget\n",
    "\n",
    "Use this interactive widget to explore recommendations for different users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Interactive recommendation widget\n",
    "\n",
    "user_widget = widgets.IntSlider(\n",
    "    value=100,\n",
    "    min=0,\n",
    "    max=NUM_USERS-1,\n",
    "    step=1,\n",
    "    description='User ID:',\n",
    "    continuous_update=False,\n",
    ")\n",
    "\n",
    "k_widget = widgets.IntSlider(\n",
    "    value=10,\n",
    "    min=1,\n",
    "    max=50,\n",
    "    step=1,\n",
    "    description='Top K:',\n",
    "    continuous_update=False,\n",
    ")\n",
    "\n",
    "rec_out = widgets.Output()\n",
    "\n",
    "def update_recommendations(user_id, k):\n",
    "    with rec_out:\n",
    "        rec_out.clear_output()\n",
    "        \n",
    "        recommendations = recommend(\n",
    "            model, user_id, k=k,\n",
    "            item_genre_features=GENRE_FEATURES,\n",
    "            item_synopsis_embeddings=SYNOPSIS_EMBEDDINGS,\n",
    "        )\n",
    "        \n",
    "        print(\"=\"*70)\n",
    "        print(f\"TOP-{k} RECOMMENDATIONS FOR USER {user_id}\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        print(f\"\\n{'Rank':<6} {'Score':<10} {'Title':<50} {'Genres'}\")\n",
    "        print(\"-\" * 100)\n",
    "        \n",
    "        for rec in recommendations:\n",
    "            title = rec['title'][:47] + '...' if len(rec['title']) > 47 else rec['title']\n",
    "            print(f\"{rec['rank']:<6} {rec['score']:.4f}     {title:<50} {rec['genres']}\")\n",
    "\n",
    "widgets.interactive(update_recommendations, user_id=user_widget, k=k_widget)\n",
    "\n",
    "display(widgets.VBox([user_widget, k_widget, rec_out]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\nThis notebook provides a complete interface for:\n\n1. **Loading trained models** from Google Drive\n2. **Single model selection** - Choose between:\n   - `NeuMF_best.pt` - Baseline (CF only)\n   - `NeuMFPlus_genre_best.pt` - Genre-enhanced\n   - `NeuMFPlus_genre_synopsis_bestt.pt` - Full model (Genre + Synopsis)\n3. **Model comparison** - Compare predictions from different models side-by-side\n4. **Predicting scores** for user-item pairs\n5. **Generating recommendations** for users\n6. **Cold-start predictions** for new movies\n7. **Interactive exploration** of the recommendation system\n\n**Tips:**\n- Use the GPU runtime in Colab for faster inference\n- Adjust `GDRIVE_BASE` if your files are in a different location\n- Ensure all data files (mappings.pkl, features, metadata) are in the correct directories\n- Use the Model Comparison section to understand how content features affect predictions\n\n**Model Selection Guide:**\n- Use **Baseline** for pure collaborative filtering (fastest)\n- Use **Genre Only** when you want genre-aware recommendations\n- Use **Genre + Synopsis** for the best accuracy (slower but most accurate)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
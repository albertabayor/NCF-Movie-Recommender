{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeuMF+ Training - Genre + Synopsis (Full Model)\n",
    "\n",
    "This notebook trains the **complete NeuMF+** model with:\n",
    "- **Collaborative Filtering** (NeuMF: GMF + MLP)\n",
    "- **Genre Features** (multi-hot encoding)\n",
    "- **Synopsis Features** (Sentence-BERT embeddings)\n",
    "- **Gated Fusion** (dynamic weighting of all signals)\n",
    "\n",
    "**Model Variants:**\n",
    "- NeuMF (baseline): Only CF interactions → HR@10 ~0.973\n",
    "- NeuMF+ (genre only): CF + Genre → HR@10 ~0.970\n",
    "- **NeuMF+ (genre + synopsis): CF + Genre + Synopsis → Expected HR@10 ~0.975+**\n",
    "\n",
    "**Prerequisites:**\n",
    "- Google Drive with: ratings.csv, movies_metadata.csv, links.csv\n",
    "- Colab Pro (recommended for longer sessions)\n",
    "- Estimated time: 2-3 hours for preprocessing, 4-6 hours for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "print(\"\\n✓ Google Drive mounted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Clone and Setup Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/albertabayor/NCF-Movie-Recommender.git\n",
    "\n",
    "import os\n",
    "os.chdir('NCF-Movie-Recommender')\n",
    "!git pull origin main\n",
    "\n",
    "# Link data\n",
    "!rm -rf data\n",
    "!ln -s \"/content/drive/MyDrive/NCF-Movie-Recommender/data\" data\n",
    "\n",
    "# Link experiments\n",
    "!mkdir -p /content/drive/MyDrive/NCF-Movie-Recommender/experiments/trained_models\n",
    "!rm -rf experiments\n",
    "!ln -s /content/drive/MyDrive/NCF-Movie-Recommender/experiments experiments\n",
    "\n",
    "# Link datasets\n",
    "!mkdir -p /content/drive/MyDrive/NCF-Movie-Recommender/datasets\n",
    "!rm -rf datasets\n",
    "!ln -s /content/drive/MyDrive/NCF-Movie-Recommender/datasets datasets\n",
    "\n",
    "# Install dependencies\n",
    "!pip install -q torch torchvision pandas numpy scikit-learn tqdm tensorboard sentence-transformers\n",
    "\n",
    "print(\"\\n✓ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Extract Synopsis Embeddings (30-60 minutes)\n",
    "\n",
    "This step:\n",
    "1. Loads movie overviews from movies_metadata.csv\n",
    "2. Converts text to 384-dimensional vectors using Sentence-BERT\n",
    "3. Saves embeddings for use in NeuMF+ training\n",
    "\n",
    "**Skip this if synopsis_embeddings.npy already exists in data/**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "# Check if synopsis embeddings already exist\n",
    "import os\n",
    "if os.path.exists('data/synopsis_embeddings.npy'):\n",
    "    print(\"Synopsis embeddings already exist! Skipping extraction.\")\n",
    "    print(\"If you want to re-extract, delete data/synopsis_embeddings.npy first.\")\n",
    "else:\n",
    "    print(\"Extracting synopsis embeddings (this takes 30-60 minutes)...\")\n",
    "    exec(open('extract_synopsis_embeddings.py').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Map Synopsis Embeddings to MovieLens movieIds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if mapping already exists\n",
    "if os.path.exists('data/item_synopsis_embeddings.npy'):\n",
    "    print(\"Synopsis mapping already exists! Skipping.\")\n",
    "else:\n",
    "    print(\"Mapping synopsis embeddings to MovieLens movieIds...\")\n",
    "    exec(open('map_synopsis_to_movieid.py').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Add Synopsis Features to Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if data already has synopsis features\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    test_df = pd.read_pickle('data/test.pkl')\n",
    "    if 'synopsis_features' in test_df.columns:\n",
    "        print(\"Data already has synopsis features! Skipping.\")\n",
    "    else:\n",
    "        print(\"Adding synopsis features to processed data...\")\n",
    "        exec(open('add_synopsis_to_data.py').read())\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Please run preprocessing first (colab_preprocess_full.ipynb)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Load and Sample Data (10% for faster training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "from src.negative_sampling import build_user_history\n",
    "from src.models.neumf_plus import NeuMFPlus\n",
    "from src.train import train_model\n",
    "\n",
    "# Load data\n",
    "train_df = pd.read_pickle('data/train.pkl')\n",
    "val_df = pd.read_pickle('data/val.pkl')\n",
    "test_df = pd.read_pickle('data/test.pkl')\n",
    "\n",
    "with open('data/mappings.pkl', 'rb') as f:\n",
    "    mappings = pickle.load(f)\n",
    "\n",
    "num_users = mappings['num_users']\n",
    "num_items = mappings['num_items']\n",
    "num_genres = mappings['num_genres']\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SAMPLING 10% OF TRAINING DATA\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nOriginal train size: {len(train_df):,}\")\n",
    "\n",
    "# Build item-to-genre mapping BEFORE sampling\n",
    "print(\"\\nBuilding item-to-genre mapping...\")\n",
    "unique_items = train_df['movieId'].unique()\n",
    "item_genre_features = np.zeros((num_items, num_genres), dtype=np.float32)\n",
    "\n",
    "for item_id in unique_items:\n",
    "    item_rows = train_df[train_df['movieId'] == item_id]\n",
    "    if len(item_rows) > 0:\n",
    "        item_genre_features[item_id] = item_rows['genre_features'].iloc[0]\n",
    "\n",
    "print(f\"  Item-to-genre mapping shape: {item_genre_features.shape}\")\n",
    "\n",
    "# Build item-to-synopsis mapping BEFORE sampling\n",
    "if 'synopsis_features' in train_df.columns:\n",
    "    print(\"\\nBuilding item-to-synopsis mapping...\")\n",
    "    item_synopsis_embeddings = np.zeros((num_items, 384), dtype=np.float32)\n",
    "    \n",
    "    for item_id in unique_items:\n",
    "        item_rows = train_df[train_df['movieId'] == item_id]\n",
    "        if len(item_rows) > 0:\n",
    "            item_synopsis_embeddings[item_id] = item_rows['synopsis_features'].iloc[0]\n",
    "    \n",
    "    print(f\"  Item-to-synopsis mapping shape: {item_synopsis_embeddings.shape}\")\n",
    "    has_synopsis = True\n",
    "else:\n",
    "    print(\"\\n⚠️  No synopsis features found in data!\")\n",
    "    print(\"   Run Steps 3-5 first to add synopsis features.\")\n",
    "    has_synopsis = False\n",
    "    item_synopsis_embeddings = None\n",
    "\n",
    "# Sample 10% of training data\n",
    "SAMPLE_RATIO = 0.10\n",
    "np.random.seed(42)\n",
    "sample_idx = np.random.choice(\n",
    "    len(train_df), \n",
    "    int(len(train_df) * SAMPLE_RATIO), \n",
    "    replace=False\n",
    ")\n",
    "train_df = train_df.iloc[sample_idx].copy()\n",
    "\n",
    "print(f\"\\nSampled train size: {len(train_df):,} ({SAMPLE_RATIO*100:.0f}%)\")\n",
    "\n",
    "train_users = train_df['userId'].values\n",
    "train_items = train_df['movieId'].values\n",
    "val_users = val_df['userId'].values\n",
    "val_items = val_df['movieId'].values\n",
    "\n",
    "# Extract validation features\n",
    "val_genre_features = np.stack(val_df['genre_features'].values)\n",
    "print(f\"\\nVal genre shape: {val_genre_features.shape}\")\n",
    "\n",
    "if has_synopsis:\n",
    "    val_synopsis_features = np.stack(val_df['synopsis_features'].values)\n",
    "    print(f\"Val synopsis shape: {val_synopsis_features.shape}\")\n",
    "else:\n",
    "    val_synopsis_features = None\n",
    "\n",
    "print(f\"\\nUsers: {num_users:,}\")\n",
    "print(f\"Items: {num_items:,}\")\n",
    "print(f\"Genres: {num_genres}\")\n",
    "print(f\"\\nTrain: {len(train_users):,} ratings\")\n",
    "print(f\"Val:   {len(val_users):,} ratings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if device == 'cuda':\n",
    "    print(f\"\\nGPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Create NeuMF+ Model (Genre + Synopsis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuMFPlus(\n",
    "    num_users=num_users,\n",
    "    num_items=num_items,\n",
    "    num_genres=num_genres,\n",
    "    # Content encoder settings\n",
    "    genre_embed_dim=64,\n",
    "    synopsis_embed_dim=384,\n",
    "    content_embed_dim=256,\n",
    "    content_encoder_dropout=0.1,\n",
    "    # Gated fusion settings\n",
    "    gated_fusion_hidden_dim=64,\n",
    "    gated_fusion_dropout=0.1,\n",
    "    # Output settings\n",
    "    output_hidden_dim=64,\n",
    "    output_dropout=0.2,\n",
    "    # Ablation study flags\n",
    "    use_genre=True,         # Enable genre features\n",
    "    use_synopsis=has_synopsis,  # Enable synopsis if available\n",
    "    use_gated_fusion=True,  # Enable gated fusion\n",
    ")\n",
    "\n",
    "model = model.to('cuda')\n",
    "param_count = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"NeuMF+ MODEL CREATED\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nModel: NeuMF+ (Neural Collaborative Filtering + Content Features)\")\n",
    "print(f\"Parameters: {param_count:,}\")\n",
    "print(f\"\\nArchitecture:\")\n",
    "print(f\"  ✓ CF Branch (NeuMF: GMF + MLP)\")\n",
    "print(f\"  ✓ Genre Encoder ({num_genres} → 64)\")\n",
    "if has_synopsis:\n",
    "    print(f\"  ✓ Synopsis Encoder (384 → 192)\")\n",
    "print(f\"  ✓ Gated Fusion (CF + Content)\")\n",
    "print(f\"\\nFeatures:\")\n",
    "print(f\"  - use_genre: True\")\n",
    "print(f\"  - use_synopsis: {has_synopsis}\")\n",
    "print(f\"  - use_gated_fusion: True\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Train NeuMF+ (Genre + Synopsis)\n",
    "\n",
    "**Configuration:**\n",
    "- Data: 10% sample (~1.8M ratings)\n",
    "- Batch size: 512\n",
    "- Learning rate: 1e-4 (reduced to prevent NaN)\n",
    "- Workers: 0 (single worker)\n",
    "- Mixed precision: FP16\n",
    "- Max epochs: 15\n",
    "- Early stopping patience: 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.train import train_model\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"NeuMF+ TRAINING - GENRE + SYNOPSIS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nGPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  - Data: 10% sample ({len(train_users):,} ratings)\")\n",
    "print(f\"  - Batch size: 512\")\n",
    "print(f\"  - Learning rate: 1e-4\")\n",
    "print(f\"  - Workers: 0\")\n",
    "print(f\"  - Mixed precision: FP16\")\n",
    "print(f\"  - Max epochs: 15\")\n",
    "print(f\"  - Early stopping patience: 3\")\n",
    "print(f\"\\nFeatures:\")\n",
    "print(f\"  - Genre: Enabled\")\n",
    "print(f\"  - Synopsis: {has_synopsis}\")\n",
    "print(f\"\\nEstimated time: 4-6 hours\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Prepare validation data\n",
    "val_data = {\n",
    "    'users': val_users,\n",
    "    'items': val_items,\n",
    "    'genre_features': val_genre_features,\n",
    "}\n",
    "\n",
    "if has_synopsis:\n",
    "    val_data['synopsis_features'] = val_synopsis_features\n",
    "\n",
    "# Train the model\n",
    "history = train_model(\n",
    "    model=model,\n",
    "    train_users=train_users,\n",
    "    train_items=train_items,\n",
    "    val_data=val_data,\n",
    "    num_items=num_items,\n",
    "    num_epochs=15,\n",
    "    batch_size=512,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=1e-5,\n",
    "    num_negatives=4,\n",
    "    device='cuda',\n",
    "    num_workers=0,\n",
    "    use_amp=True,\n",
    "    save_dir='./experiments/trained_models',\n",
    "    early_stopping_patience=3,\n",
    "    early_stopping_metric='hr@10',\n",
    "    lr_scheduler_patience=2,\n",
    "    lr_scheduler_factor=0.5,\n",
    "    log_dir='./experiments/logs/tensorboard',\n",
    "    item_genre_features=item_genre_features,\n",
    "    item_synopsis_features=item_synopsis_embeddings,  # Add this parameter\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nBest HR@10: {max([m.get('hr@10', 0) for m in history['val_metrics']]):.4f}\")\n",
    "print(f\"\\nModel saved to: experiments/trained_models/\")\n",
    "print(\"Also saved to Google Drive for persistence.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**What happened:**\n",
    "1. ✅ Extracted synopsis embeddings using Sentence-BERT (if not exists)\n",
    "2. ✅ Mapped embeddings to MovieLens movieIds\n",
    "3. ✅ Added synopsis features to processed data\n",
    "4. ✅ Created NeuMF+ with genre + synopsis\n",
    "5. ✅ Trained on T4/L4 GPU with mixed precision\n",
    "6. ✅ Saved best model to Google Drive\n",
    "\n",
    "**Model Comparison:**\n",
    "| Model | Features | HR@10 |\n",
    "|-------|----------|-------|\n",
    "| NeuMF | CF only | ~0.973 |\n",
    "| NeuMF+ | CF + Genre | ~0.970 |\n",
    "| NeuMF+ | CF + Genre + Synopsis | **Expected: ~0.975+** |\n",
    "\n",
    "**Next steps:**\n",
    "- Compare HR@10 across all model variants\n",
    "- Use best model for recommendations\n",
    "- Analyze gate values to understand CF vs content contribution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
